{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFVI for conditional density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Uniform\n",
    "from observations import mnist\n",
    "%matplotlib inline\n",
    "\n",
    "ed.set_seed(44)\n",
    "data_dir = \"/tmp/data\"\n",
    "out_dir = \"/tmp/out\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "def generator(array, batch_size):\n",
    "    \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "    start = 0  # pointer to where we are in iteration\n",
    "    while True:\n",
    "        stop = start + batch_size\n",
    "        diff = stop - array.shape[0]\n",
    "        if diff <= 0:\n",
    "            batch = array[start:stop]\n",
    "            start += batch_size\n",
    "        else:\n",
    "            batch = np.concatenate((array[start:], array[:diff]))\n",
    "            start = diff\n",
    "            batch = batch.astype(np.float32) \n",
    "            #batch = np.random.binomial(1, batch)  # binarize images\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define experiment, generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, M = 1000, 1000  # batch size during training\n",
    "D, K = 1, 1\n",
    "\n",
    "def gen_z(N):\n",
    "    return np.random.normal(size=(N,K))/2.\n",
    "def gen_x(z):\n",
    "    N = z.shape[0]\n",
    "    return np.exp(z)/2. - 1/2. + np.random.normal(size=(N,D))/25.\n",
    "\n",
    "z_train, z_test = gen_z(N), gen_z(N)\n",
    "x_train, x_test = gen_x(z_train), gen_x(z_test)\n",
    "x_train_generator = generator(x_train, M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate experiment to Edward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, PointMass\n",
    "\n",
    "# define log-ratio estimator r(x,z) (has to be flexible enough or nothing works!)\n",
    "def discriminative_network(xdict, zdict, betadict):\n",
    "    \"\"\"Outputs probability in logits.\"\"\"\n",
    "    net = tf.layers.dense(tf.concat([xdict[x], zdict[z]], 1), 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 1, activation=None)\n",
    "    return net\n",
    "\n",
    "\n",
    "# define simple generative model\n",
    "z = Normal(loc=tf.zeros([M, K]), scale=tf.ones([M,K])/2.)  # p(z)\n",
    "x = Normal(loc= tf.exp(z)/2. - 0.5, scale=tf.ones([M,D])/25.)    # p(x|z)\n",
    "\n",
    "# define simple flexible recognition model q(z|x)\n",
    "def generative_network(eps):\n",
    "    net = tf.layers.dense(eps, 5, activation=tf.nn.tanh)\n",
    "    net = tf.layers.dense(net, 5, activation=tf.nn.tanh)\n",
    "    net = tf.layers.dense(net, D, activation=None)\n",
    "    #net = tf.layers.dense(eps, D, activation=None)\n",
    "    return net\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, [M, D]) # container for x_train\n",
    "qz = Normal(loc=generative_network(x_ph), scale=tf.ones([K])/10.) # 'single-component MDN with fixed covariance'\n",
    "#qz = Normal(loc=tf.log(2 * x_ph + 1), scale=tf.ones([M,D])/10.) # this would be the target solution\n",
    "\n",
    "\n",
    "# ImplicitKLqp does everything ...\n",
    "inference = ed.ImplicitKLqp(latent_vars={z: qz}, \n",
    "                            data={x: x_ph}, \n",
    "                            discriminator=discriminative_network,\n",
    "                            global_vars=None)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9)   # optimizer for q(z|x)\n",
    "optimizer_d = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9) # optimizer for r(x,z)\n",
    "\n",
    "inference.initialize(\n",
    "    optimizer=optimizer, optimizer_d=optimizer_d,\n",
    "    global_step = None, #tf.Variable(0, trainable=False, name=\"global_step\"),\n",
    "    global_step_d = None, #tf.Variable(0, trainable=False, name=\"global_step_d\"),    \n",
    "    n_iter=10000, n_print=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start session\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize initial state of density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(x_test.shape[0], M, replace=False)\n",
    "plt.plot(x_test[idx], qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}), 'bo')\n",
    "plt.plot(x_test[idx], z_test[idx], 'ko')\n",
    "\n",
    "plt.title('q(z | x) before training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.legend(['est.', 'true'])\n",
    "plt.show()\n",
    "\n",
    "np.corrcoef(z_test[idx].T, qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}).T)[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit conditional density model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for t in range(inference.n_iter):\n",
    "\n",
    "    x_batch = next(x_train_generator)\n",
    "    info_dict = inference.update(feed_dict={x_ph: x_batch})\n",
    "    inference.print_progress(info_dict)\n",
    "    \n",
    "plt.plot(x_test[idx], qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}), 'bo')\n",
    "plt.plot(x_test[idx], z_test[idx], 'ko')\n",
    "plt.title('q(z | x) after training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.legend(['est.', 'true'])\n",
    "plt.show()\n",
    "\n",
    "np.corrcoef(z_test[idx].T, qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}).T)[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference.increment_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six\n",
    "from edward.util import check_latent_vars, copy, get_session\n",
    "from edward.models import RandomVariable\n",
    "\n",
    "sess = ed.get_session()\n",
    "discriminator=discriminative_network\n",
    "\n",
    "scope = tf.get_default_graph().unique_name(\"inference\")\n",
    "latent_vars={z: qz}\n",
    "data = {x: z}\n",
    "qbeta_sample = {}\n",
    "pz_sample = {}\n",
    "qz_sample = {}\n",
    "for z, qz in six.iteritems(latent_vars):\n",
    "    # Copy local variables p(z), q(z) to draw samples\n",
    "    # z' ~ p(z | beta'), z' ~ q(z | beta').\n",
    "    pz_copy = copy(z, dict_swap=qbeta_sample, scope=scope)\n",
    "    pz_sample[z] = pz_copy.value()\n",
    "    qz_sample[z] = qz.value()\n",
    "\n",
    "# Collect x' ~ p(x | z', beta') and x' ~ q(x).\n",
    "dict_swap = qbeta_sample.copy()\n",
    "dict_swap.update(qz_sample)\n",
    "x_psample = {}\n",
    "x_qsample = {}\n",
    "for x, x_data in six.iteritems(data):\n",
    "    if isinstance(x, tf.Tensor):\n",
    "        if \"Placeholder\" not in x.op.type:\n",
    "            # Copy p(x | z, beta) to get draw p(x | z', beta').\n",
    "            x_copy = copy(x, dict_swap=dict_swap, scope=scope)\n",
    "            x_psample[x] = x_copy\n",
    "            x_qsample[x] = x_data\n",
    "    elif isinstance(x, RandomVariable):\n",
    "        # Copy p(x | z, beta) to get draw p(x | z', beta').\n",
    "        x_copy = copy(x, dict_swap=dict_swap, scope=scope)\n",
    "        x_psample[x] = x_copy.value()\n",
    "        x_qsample[x] = x_data    \n",
    "\n",
    "with tf.variable_scope(\"Disc\"):\n",
    "    r_psample = discriminator(x_psample, pz_sample, qbeta_sample)        \n",
    "        \n",
    "tf.global_variables_initializer().run()\n",
    "        \n",
    "        \n",
    "x_batch_ = next(x_train_generator)\n",
    "plt.plot(x_batch_, qz.eval(session=sess, feed_dict={x_ph: x_batch_}), 'bo')\n",
    "plt.show()    \n",
    "    \n",
    "x_batch = x_train[:100]\n",
    "plt.plot(x_batch, x_psample[x].eval(feed_dict={x_ph : x_train[:100]}), 'bo')\n",
    "plt.show()\n",
    "\n",
    "x_batch = x_train[:100]\n",
    "plt.plot(z_train[:100], pz_sample[z].eval(feed_dict={x_ph : z_train[:100]}), 'bo')\n",
    "plt.show()\n",
    "\n",
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_psample = discriminator(x_psample, pz_sample, qbeta_sample)\n",
    "    rs = r_psample.eval(feed_dict={x_ph : x_train[:100]})\n",
    "    pz_sample[z].eval(feed_dict={x_ph : z_train[:100]})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_qsample = inference.discriminator(x_psample, pz_sample, qbeta_sample)\n",
    "    rs = r_qsample.eval(feed_dict={x_ph : x_train})\n",
    "    pz_sample[z].eval(feed_dict={x_ph : x_train})\n",
    "    \n",
    "plt.plot(x_batch, rs, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_qsample = inference.discriminator(x_qsample, qz_sample, qbeta_sample)\n",
    "    rs = r_qsample.eval(feed_dict={x_ph : x_train})\n",
    "    pz_sample[z].eval(feed_dict={x_ph : x_train})\n",
    "    \n",
    "plt.plot(x_batch, rs, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(x_test, z_test, 'ko')\n",
    "x_batch = next(x_train_generator)\n",
    "x_batch[0,0] = 10\n",
    "qs = np.zeros(1000)\n",
    "for i in range(1000):\n",
    "    qs[i] = qz.eval(session=sess, feed_dict={x_ph : x_batch})[0,0]\n",
    "plt.hist(qs)\n",
    "#plt.hist(z_train, bins=np.linspace(-1, 3, 30))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
