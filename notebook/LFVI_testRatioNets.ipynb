{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFVI for conditional density estimation\n",
    "\n",
    "- testing the log-denisty-ratio network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Uniform\n",
    "from observations import mnist\n",
    "%matplotlib inline\n",
    "\n",
    "ed.set_seed(44)\n",
    "data_dir = \"/tmp/data\"\n",
    "out_dir = \"/tmp/out\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "def generator(array, batch_size):\n",
    "    \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "    start = 0  # pointer to where we are in iteration\n",
    "    while True:\n",
    "        stop = start + batch_size\n",
    "        diff = stop - array.shape[0]\n",
    "        if diff <= 0:\n",
    "            batch = array[start:stop]\n",
    "            start += batch_size\n",
    "        else:\n",
    "            batch = np.concatenate((array[start:], array[:diff]))\n",
    "            start = diff\n",
    "            batch = batch.astype(np.float32) \n",
    "            #batch = np.random.binomial(1, batch)  # binarize images\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define experiment, generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, M = 1000, 1000  # batch size during training\n",
    "D, K = 1, 1\n",
    "\n",
    "def gen_z(N):\n",
    "    return np.random.normal(size=(N,K))/2.\n",
    "def gen_x(z):\n",
    "    N = z.shape[0]\n",
    "    return np.exp(z)/2. - 1/2. + np.random.normal(size=(N,D))/25.\n",
    "\n",
    "z_train, z_test = gen_z(N), gen_z(N)\n",
    "x_train, x_test = gen_x(z_train), gen_x(z_test)\n",
    "x_train_generator = generator(x_train, M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate experiment to Edward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, PointMass\n",
    "\n",
    "# define log-ratio estimator r(x,z) (has to be flexible enough or nothing works!)\n",
    "def discriminative_network(xdict, zdict, betadict):\n",
    "    \"\"\"Outputs probability in logits.\"\"\"\n",
    "    net = tf.layers.dense(tf.concat([xdict[x], zdict[z]], 1), 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 1, activation=None)\n",
    "    return net\n",
    "\n",
    "\n",
    "# define simple generative model\n",
    "z = Normal(loc=tf.zeros([M, K]), scale=tf.ones([M,K])/2.)  # p(z)\n",
    "x = Normal(loc= tf.exp(z)/2. - 0.5, scale=tf.ones([M,D])/25.)    # p(x|z)\n",
    "\n",
    "# defome recognition model\n",
    "x_ph = tf.placeholder(tf.float32, [M, D]) # container for x_train/x_test\n",
    "z_ph = tf.placeholder(tf.float32, [M, K]) # container for z_train/z_train\n",
    "#qz = Normal(loc=tf.log(2 * x_ph + 1), scale=tf.ones([M,D])/10.) # this would be the target solution\n",
    "qz = Normal(loc=-tf.cos(3*x_ph), scale=tf.ones([M,D])/10.) # this would be the target solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import six\n",
    "from edward.inferences.gan_inference import GANInference\n",
    "from edward.models import RandomVariable\n",
    "from edward.util import check_latent_vars, copy, get_session\n",
    "\n",
    "scope = tf.get_default_graph().unique_name(\"inference\")\n",
    "discriminator=discriminative_network\n",
    "qbeta_sample = {}\n",
    "x_qsample, x_psample = {x : x_ph},        {x: copy(x, dict_swap=qbeta_sample, scope=scope).value()}\n",
    "qz_sample, pz_sample = {z: qz.value()},   {z: copy(z, dict_swap=qbeta_sample, scope=scope).value()}\n",
    "\n",
    "px_test, pz_test = {x: x_ph}, {z: z_ph}\n",
    "\n",
    "with tf.variable_scope(\"Disc\"):\n",
    "    r_qsample = discriminator(x_qsample, qz_sample, qbeta_sample)\n",
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_psample = discriminator(x_psample, pz_sample, qbeta_sample)\n",
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_test = discriminator(px_test, pz_test, qbeta_sample)\n",
    "    \n",
    "\n",
    "def log_loss(psample, qsample):\n",
    "    \"\"\"Point-wise log loss.\"\"\"\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.ones_like(psample), logits=psample) + \\\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(qsample), logits=qsample)\n",
    "    return loss\n",
    "ratio_loss = log_loss\n",
    "loss_d = tf.reduce_mean(ratio_loss(r_psample, r_qsample))\n",
    "\n",
    "var_list_d = tf.get_collection(\n",
    "tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Disc\")\n",
    "print('var_list_d', var_list_d)\n",
    "\n",
    "optimizer_d = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9) # optimizer for r(x,z)\n",
    "\n",
    "grads_d = tf.gradients(loss_d, var_list_d)\n",
    "grads_and_vars_d = list(zip(grads_d, var_list_d))\n",
    "train_d = optimizer_d.apply_gradients(grads_and_vars_d,\n",
    "global_step=tf.Variable(0, trainable=False, name=\"global_step_d\"))\n",
    "\n",
    "increment_t = tf.Variable(0, trainable=False, name=\"iteration\").assign_add(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start session\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "feed_dict = {x_ph : x_train}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lss = np.zeros(10000)\n",
    "for i in range(lss.size):\n",
    "    _, t, loss_d_ = sess.run(\n",
    "              [train_d, increment_t, loss_d], feed_dict=feed_dict)\n",
    "    lss[i] = loss_d_\n",
    "    \n",
    "plt.semilogx(lss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gx, gz = np.meshgrid(np.linspace(-1., 2., 25), np.linspace(-1.5, 1.5, 40))\n",
    "\n",
    "feed_dict = {x_ph: gx.reshape(-1,1),\n",
    "             z_ph: gz.reshape(-1,1)}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,2,1)\n",
    "idx = np.random.choice(x_test.shape[0], M, replace=False)\n",
    "plt.plot(x_test[idx], qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}), 'bo')\n",
    "plt.plot(x_test[idx], z_test[idx], 'ko')\n",
    "plt.title('q(z | x) before training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.legend(['est.', 'true'], loc=4)\n",
    "plt.axis([-1, 2, -1.5, 1.5])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow( np.flipud(r_test.eval(session=sess, feed_dict=feed_dict).reshape(40,25)) ) \n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "np.corrcoef(z_test[idx].T, qz.eval(session=sess, feed_dict={x_ph: x_test[idx]}).T)[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# true model (up to covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# offset +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# offset -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# positive slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncorrelated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# negative slope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
