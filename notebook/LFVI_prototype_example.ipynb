{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFVI for conditional density estimation\n",
    "\n",
    "Notebook for testing LFVI as a tool for non-parametric posterior densitiy estimation in ABC\n",
    "\n",
    "Notation:\n",
    "- $x$ is the observed variable and corresponds to the summary statistics used in ABC\n",
    "- $z$ is the latent variable and corresponds to the simulator parameter ( '' $\\theta$ '' ) in ABC\n",
    "\n",
    "We seek to learn $q(z|x) \\approx p(z|x)$ for prior $p(z)$ and some imlicitly defined likelihood $p(x|z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define experiment, generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, M = 1000, 100  # batch size during training\n",
    "D, K = 1, 1       # dimensionality of observed and hidden variables\n",
    "\n",
    "# prior \n",
    "def gen_z(N):\n",
    "    return 3 * (np.random.uniform(size=(N,K))-0.5)\n",
    "\n",
    "# simulator (induces implicit likelihood)\n",
    "def gen_x(z):\n",
    "    N = z.shape[0]\n",
    "    return np.exp(z)/2. - 1/2. + np.random.normal(size=(N,D))/16.\n",
    "\n",
    "# train and test data (test data only used for visualization)\n",
    "z_train, z_test = gen_z(N), gen_z(1000)\n",
    "x_train, x_test = gen_x(z_train), gen_x(z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate experiment to Edward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, Uniform, PointMass\n",
    "\n",
    "# define log-ratio estimator r(x,z) (has to be flexible enough or nothing works!)\n",
    "def discriminative_network(xdict, zdict, betadict):\n",
    "    \"\"\"Outputs probability in logits.\"\"\"\n",
    "    net = tf.layers.dense(tf.concat([xdict[x], zdict[z]], 1), 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 64, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 1, activation=None)\n",
    "    return net\n",
    "\n",
    "# define simple generative model in Edward (has to match data-generation mechanism above)\n",
    "z = Uniform(low=-1.5*tf.ones([M,K]), high=1.5*tf.ones([M,K])) \n",
    "x = Normal(loc= tf.exp(z)/2. - 0.5, scale=tf.ones([M,D])/16.)    # p(x|z)\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, [M, D]) # container for x_train\n",
    "z_ph = tf.placeholder(tf.float32, [M, K]) # container for z_train\n",
    "\n",
    "# define simple flexible recognition model q(z|x)\n",
    "def generative_network(eps):\n",
    "    net = tf.layers.dense(eps, 5, activation=tf.nn.tanh)\n",
    "    net = tf.layers.dense(net, 5, activation=tf.nn.tanh)\n",
    "    net = tf.layers.dense(net, D, activation=None)\n",
    "    return net\n",
    "# 'single-component MDN with fixed covariance'\n",
    "qz = Normal(loc=generative_network(x_ph), scale=tf.ones([M,D])/10.)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some Edward / tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import six\n",
    "from edward.util import check_latent_vars, copy, get_session\n",
    "\n",
    "## assemble elements of tensorflow graph \n",
    "\n",
    "scope = tf.get_default_graph().unique_name(\"inference\")\n",
    "discriminator=discriminative_network\n",
    "qbeta_sample = {}\n",
    "x_qsample, x_psample = {x : x_ph},        {x: copy(x, dict_swap=qbeta_sample, scope=scope).value()}\n",
    "qz_sample, pz_sample = {z: qz.value()},   {z: copy(z, dict_swap=qbeta_sample, scope=scope).value()}\n",
    "\n",
    "x_ph_t = tf.placeholder(tf.float32, [1000, D]) # container for x_test\n",
    "z_ph_t = tf.placeholder(tf.float32, [1000, K]) # container for z_train\n",
    "px_test, pz_test = {x: x_ph_t}, {z: z_ph_t}\n",
    "\n",
    "with tf.variable_scope(\"Disc\"):\n",
    "    r_qsample = discriminator(x_qsample, qz_sample, qbeta_sample) # see implicit_klqp\n",
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_psample = discriminator(x_psample, pz_sample, qbeta_sample) # for original usage\n",
    "with tf.variable_scope(\"Disc\", reuse=True):\n",
    "    r_test = discriminator(px_test, pz_test, qbeta_sample)          # for querying the log-ratio density net\n",
    "    \n",
    "\n",
    "## construct losses and optimizers\n",
    "    \n",
    "# loss and trainer for ratio-density network     \n",
    "def log_loss(psample, qsample):\n",
    "    \"\"\"Point-wise log loss.\"\"\"\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.ones_like(psample), logits=psample) + \\\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(qsample), logits=qsample)\n",
    "    return loss\n",
    "ratio_loss = log_loss\n",
    "loss_d = tf.reduce_mean(ratio_loss(r_psample, r_qsample)) \n",
    "\n",
    "var_list_d = tf.get_collection(\n",
    "tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Disc\")\n",
    "print('var_list_d \\n', var_list_d)\n",
    "\n",
    "optimizer_d = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9) # optimizer for r(x,z)\n",
    "\n",
    "grads_d = tf.gradients(loss_d, var_list_d)\n",
    "grads_and_vars_d = list(zip(grads_d, var_list_d))\n",
    "train_d = optimizer_d.apply_gradients(grads_and_vars_d,\n",
    "global_step=tf.Variable(0, trainable=False, name=\"global_step_d\"))\n",
    "increment_t_d = tf.Variable(0, trainable=False, name=\"iteration_d\").assign_add(1)\n",
    "\n",
    "# loss and trainer for recognition network     \n",
    "reg_terms_all = tf.losses.get_regularization_losses()\n",
    "reg_terms = [r for r in reg_terms_all if r not in reg_terms_d]\n",
    "scale = 1.0\n",
    "scaled_ratio = tf.reduce_sum(scale * r_qsample)\n",
    "pbeta_log_prob = 0.0\n",
    "qbeta_log_prob = 0.0\n",
    "loss = -(pbeta_log_prob - qbeta_log_prob + scaled_ratio -\n",
    "         tf.reduce_sum(reg_terms))\n",
    "var_list = [v for v in tf.trainable_variables() if v not in var_list_d]\n",
    "print('var_list \\n', var_list)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9) # optimizer for r(x,z)\n",
    "\n",
    "grads = tf.gradients(loss, var_list)\n",
    "grads_and_vars = list(zip(grads, var_list))\n",
    "train = optimizer.apply_gradients(grads_and_vars,\n",
    "global_step=tf.Variable(0, trainable=False, name=\"global_step\"))\n",
    "increment_t = tf.Variable(0, trainable=False, name=\"iteration\").assign_add(1)\n",
    "\n",
    "\n",
    "## start session\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "feed_dict = {x_ph : x_train}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execute and visualize\n",
    "\n",
    "- task is to match the samples of $q(z|x) q(x)$ and $p(z,x)$\n",
    "- for $q(x)$ being the empirical distribution, this is achieved when $q(z|x) \\approx p(z|x)$\n",
    "- we're done once the samples from $q(z|x)$ (blue dots) match the samples from $p(z|x)$ (black dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,14))    \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt_axes = [-1, 2, -1.5, 1.5]\n",
    "plt_idx = np.random.choice(x_test.shape[0], M, replace=False)\n",
    "plt.plot(x_test[plt_idx], qz.eval(session=sess, feed_dict={x_ph: x_test[plt_idx]}), 'bo')\n",
    "plt.plot(x_test[plt_idx], z_test[plt_idx], 'ko')\n",
    "plt.title('q(z | x) before training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.legend(['est.', 'true'], loc=4)\n",
    "plt.axis([-1, 2, -1.5, 1.5])\n",
    "\n",
    "\n",
    "\n",
    "## actual fitting \n",
    "\n",
    "tstart = time.time()\n",
    "lss_d = np.zeros(10000)\n",
    "lss = np.zeros(10000)\n",
    "for i in range(lss_d.size):\n",
    "    \n",
    "    idx = np.random.choice(N, M, replace=False)\n",
    "    feed_dict = {x_ph : x_train[idx]}\n",
    "\n",
    "    _, t, loss_d_ = sess.run(\n",
    "              [train_d, increment_t_d, loss_d], feed_dict=feed_dict)\n",
    "    lss_d[i] = loss_d_\n",
    "    \n",
    "    _, t, loss_ = sess.run(\n",
    "              [train, increment_t, loss], feed_dict=feed_dict)    \n",
    "    lss[i] = loss_\n",
    "    \n",
    "dur = time.time() - tstart\n",
    "\n",
    "print('fitting took ' + str(dur) + 's')\n",
    " \n",
    "    \n",
    "    \n",
    "plt.subplot(4,2,2)\n",
    "plt.semilogx(lss_d)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('discriminator loss')\n",
    "plt.subplot(4,2,4)\n",
    "plt.semilogx(lss)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('recognition loss')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt_axes = [-1, 2, -1.5, 1.5]\n",
    "idx = np.random.choice(x_test.shape[0], M, replace=False)\n",
    "plt.plot(x_test[plt_idx], qz.eval(session=sess, feed_dict={x_ph: x_test[plt_idx]}), 'bo')\n",
    "plt.plot(x_test[plt_idx], z_test[plt_idx], 'ko')\n",
    "plt.title('q(z | x) after training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.legend(['est.', 'true'], loc=4)\n",
    "plt.axis([-1, 2, -1.5, 1.5])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "gx, gz = np.meshgrid(np.linspace(plt_axes[0], plt_axes[1], 25), np.linspace(plt_axes[2], plt_axes[3], 40))\n",
    "feed_dict_r = {x_ph_t: gx.reshape(-1,1), z_ph_t: gz.reshape(-1,1)}\n",
    "plt.imshow( np.flipud(r_test.eval(session=sess, feed_dict=feed_dict_r).reshape(40,25)), aspect='auto' ) \n",
    "plt.colorbar()\n",
    "plt.title('learned log-density ratio r(x,z)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.savefig('lfvi_prototype_logGaussianToyExample.pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
